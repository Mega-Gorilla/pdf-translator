{
  "version": "1.0.0",
  "metadata": {
    "source_file": "sample_llama.pdf",
    "source_lang": "en",
    "target_lang": "ja",
    "translated_at": "2026-01-15T10:48:55.923540",
    "translator_backend": "google",
    "page_count": 1,
    "paragraph_count": 13,
    "translated_count": 9
  },
  "summary": {
    "title": "LLaMA: Open and Efficient Foundation Language Models",
    "title_translated": "LLaMA: オープンで効率的な基礎言語モデル",
    "abstract": "We introduce LLaMA, a collection of founda- tion language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly avail- able datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA- 65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community1 .",
    "abstract_translated": "7B から 65B のパラメータにわたる基礎言語モデルのコレクションである LLaMA を紹介します。私たちは何兆ものトークンでモデルをトレーニングし、独自のアクセスできないデータセットに頼ることなく、公開されているデータセットのみを使用して最先端のモデルをトレーニングできることを示しました。特に、LLaMA-13B はほとんどのベンチマークで GPT-3 (175B) を上回り、LLaMA-65B は最高のモデルである Chinchilla-70B および PaLM-540B と競合します。私たちはすべてのモデルを研究コミュニティに公開しています1。",
    "organization": "Meta AI",
    "summary": "The objective of this research was to develop a collection of high-performing foundation language models, ranging from 7B to 65B parameters, that optimize performance for a limited inference budget. To achieve this, the researchers trained their models on trillions of tokens using exclusively publicly available datasets, challenging the assumption that state-of-the-art results require proprietary data or massive parameter counts. The study found that smaller models trained on more data can outperform much larger counterparts; for instance, LLaMA-13B surpassed the 175B-parameter GPT-3 on most benchmarks. Ultimately, the authors conclude that LLaMA-65B is competitive with top-tier models like PaLM-540B, and they released the collection to the research community to democratize access to efficient, large-scale language modeling.",
    "summary_translated": "この研究の目的は、限られた推論予算でパフォーマンスを最適化する、7B から 65B のパラメーターにわたる高性能の基礎言語モデルのコレクションを開発することでした。これを達成するために、研究者らは独占的に公開されているデータセットを使用して数兆のトークンでモデルをトレーニングし、最先端の結果には独自のデータや膨大なパラメータ数が必要であるという仮定に挑戦しました。この研究では、より多くのデータでトレーニングされた小規模なモデルが、はるかに大規模なモデルよりも優れたパフォーマンスを発揮できることがわかりました。たとえば、LLaMA-13B は、ほとんどのベンチマークで 175B パラメータの GPT-3 を上回りました。最終的に著者らは、LLaMA-65B が PaLM-540B のような最上位モデルと競合できると結論付け、効率的で大規模な言語モデリングへのアクセスを民主化するために、このコレクションを研究コミュニティにリリースしました。",
    "thumbnail_path": "test4_translated_thumbnail.png",
    "thumbnail_width": 400,
    "thumbnail_height": 566,
    "page_count": 1,
    "source_lang": "en",
    "target_lang": "ja",
    "title_source": "layout",
    "abstract_source": "layout"
  },
  "paragraphs": [
    {
      "id": "para_p0_b0",
      "page_number": 0,
      "text": "LLaMA: Open and Efficient Foundation Language Models",
      "block_bbox": {
        "x0": 117.27200317382812,
        "y0": 751.2919921875,
        "x1": 478.0072021484375,
        "y1": 769.9563598632812
      },
      "line_count": 1,
      "original_font_size": 14.3,
      "is_bold": true,
      "is_italic": false,
      "rotation": 0.0,
      "alignment": "center",
      "layout_block_id": "42dd46d2-0f32-41cb-8ec4-e89bf4894e5e",
      "category": "doc_title",
      "category_confidence": 0.9415737986564636,
      "font_name": "NimbusRomNo9L-Medi"
    },
    {
      "id": "para_p0_b1",
      "page_number": 0,
      "text": "Hugo Touvron∗ , Thibaut Lavril∗ , Gautier Izacard∗ , Xavier Martinet Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozière, Naman Goyal Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin Edouard Grave∗ , Guillaume Lample∗",
      "block_bbox": {
        "x0": 111.01296997070312,
        "y0": 669.92431640625,
        "x1": 484.2662658691406,
        "y1": 728.6470336914062
      },
      "line_count": 8,
      "original_font_size": 12.0,
      "is_bold": true,
      "is_italic": false,
      "rotation": 0.0,
      "alignment": "left",
      "layout_block_id": "4521efdb-b5bf-4460-a046-25e265061d11",
      "category": "text",
      "category_confidence": 0.9733920693397522,
      "translated_text": "ユーゴ・トゥヴロン∗ 、ティボー・ラブリル∗ 、ゴーティエ・イザカール∗ 、ザビエ・マルティネ・マリー＝アンヌ・ラショー、ティモシー・ラクロワ、バティスト・ロジエール、ナマン・ゴヤル エリック・ハンブロ、ファイサル・アズハル、オーレリアン・ロドリゲス、アルマン・ジュラン・エドゥアール・グラーヴ∗、ギョーム・ランプル∗",
      "font_name": "NimbusRomNo9L-Medi"
    },
    {
      "id": "para_p0_b2",
      "page_number": 0,
      "text": "Meta AI",
      "block_bbox": {
        "x0": 277.552978515625,
        "y0": 648.2825927734375,
        "x1": 317.7224426269531,
        "y1": 662.6886596679688
      },
      "line_count": 1,
      "original_font_size": 12.0,
      "is_bold": false,
      "is_italic": false,
      "rotation": 0.0,
      "alignment": "center",
      "layout_block_id": "c437236b-0d7f-4ebe-9a88-b548017081ec",
      "category": "text",
      "category_confidence": 0.9053331613540649,
      "translated_text": "メタAI",
      "font_name": "NimbusRomNo9L-Regu"
    },
    {
      "id": "para_p0_b3",
      "page_number": 0,
      "text": "Abstract",
      "block_bbox": {
        "x0": 157.7579803466797,
        "y0": 614.1453247070312,
        "x1": 202.24327087402344,
        "y1": 629.6990356445312
      },
      "line_count": 1,
      "original_font_size": 12.0,
      "is_bold": true,
      "is_italic": false,
      "rotation": 0.0,
      "alignment": "center",
      "layout_block_id": "363b8373-3bed-447a-9fcc-0dba6c44921e",
      "category": "paragraph_title",
      "category_confidence": 0.9132309556007385,
      "font_name": "NimbusRomNo9L-Medi"
    },
    {
      "id": "para_p0_b9",
      "page_number": 0,
      "text": "performance, a smaller one trained longer will ultimately be cheaper at inference. For instance, although Hoffmann et al. (2022) recommends training a 10B model on 200B tokens, we find that the performance of a 7B model continues to improve even after 1T tokens.",
      "block_bbox": {
        "x0": 306.1419982910156,
        "y0": 547.4105834960938,
        "x1": 525.7723388671875,
        "y1": 628.302001953125
      },
      "line_count": 6,
      "original_font_size": 10.9,
      "is_bold": false,
      "is_italic": false,
      "rotation": 0.0,
      "alignment": "left",
      "layout_block_id": "2995588b-2fcd-41ea-b848-6cd246d05cf3",
      "category": "text",
      "category_confidence": 0.9785284399986267,
      "translated_text": "パフォーマンスに影響を与えるため、より長くトレーニングされた小規模なものほど、最終的には推論のコストが低くなります。たとえば、Hoffmann et al. (2022) は 200B トークンで 10B モデルをトレーニングすることを推奨していますが、7B モデルのパフォーマンスは 1T トークンの後でも向上し続けることがわかりました。",
      "font_name": "NimbusRomNo9L-Regu"
    },
    {
      "id": "para_p0_b4",
      "page_number": 0,
      "text": "We introduce LLaMA, a collection of founda- tion language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly avail- able datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA- 65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community1 .",
      "block_bbox": {
        "x0": 87.54497528076172,
        "y0": 462.4625244140625,
        "x1": 273.776123046875,
        "y1": 605.9735107421875
      },
      "line_count": 13,
      "original_font_size": 10.0,
      "is_bold": false,
      "is_italic": false,
      "rotation": 0.0,
      "alignment": "left",
      "layout_block_id": "fb74289a-68e2-42fc-9c56-c61e91934dcc",
      "category": "abstract",
      "category_confidence": 0.9836386442184448,
      "translated_text": "7B から 65B のパラメータにわたる基礎言語モデルのコレクションである LLaMA を紹介します。私たちは何兆ものトークンでモデルをトレーニングし、独自のアクセスできないデータセットに頼ることなく、公開されているデータセットのみを使用して最先端のモデルをトレーニングできることを示しました。特に、LLaMA-13B はほとんどのベンチマークで GPT-3 (175B) を上回り、LLaMA-65B は最高のモデルである Chinchilla-70B および PaLM-540B と競合します。私たちはすべてのモデルを研究コミュニティに公開しています1。",
      "font_name": "NimbusRomNo9L-Regu"
    },
    {
      "id": "para_p0_b14",
      "page_number": 0,
      "text": "arXiv:2302.13971v1 [cs.CL] 27 Feb 2023",
      "block_bbox": {
        "x0": 18.34000015258789,
        "y0": 232.63002014160156,
        "x1": 35.099998474121094,
        "y1": 577.1500244140625
      },
      "line_count": 1,
      "original_font_size": 20.0,
      "is_bold": false,
      "is_italic": false,
      "rotation": 270.00000068324533,
      "alignment": "left",
      "layout_block_id": "293cb507-5fa8-4e5d-b66d-72208943e302",
      "category": "aside_text",
      "category_confidence": 0.9396297931671143,
      "translated_text": "arXiv:2302.13971v1 [cs.CL] 2023 年 2 月 27 日",
      "font_name": "Times-Roman"
    },
    {
      "id": "para_p0_b10",
      "page_number": 0,
      "text": "The focus of this work is to train a series of language models that achieve the best possible per- formance at various inference budgets, by training on more tokens than what is typically used. The resulting models, called LLaMA, ranges from 7B to 65B parameters with competitive performance compared to the best existing LLMs. For instance, LLaMA-13B outperforms GPT-3 on most bench- marks, despite being 10× smaller. We believe that this model will help democratize the access and study of LLMs, since it can be run on a single GPU. At the higher-end of the scale, our 65B-parameter model is also competitive with the best large lan- guage models such as Chinchilla or PaLM-540B.",
      "block_bbox": {
        "x0": 305.7489929199219,
        "y0": 346.8565673828125,
        "x1": 526.3759765625,
        "y1": 536.1420288085938
      },
      "line_count": 14,
      "original_font_size": 10.9,
      "is_bold": false,
      "is_italic": false,
      "rotation": 0.0,
      "alignment": "left",
      "layout_block_id": "7707f5cc-0a28-4b42-aca6-eba35f33fdb2",
      "category": "text",
      "category_confidence": 0.9822850227355957,
      "translated_text": "この研究の焦点は、通常使用されるトークンよりも多くのトークンでトレーニングすることにより、さまざまな推論予算で可能な限り最高のパフォーマンスを達成する一連の言語モデルをトレーニングすることです。結果として得られる LLaMA と呼ばれるモデルは、既存の最高の LLM と比較して優れたパフォーマンスを備えた 7B ～ 65B パラメーターの範囲にあります。たとえば、LLaMA-13B は、10 倍小さいにもかかわらず、ほとんどのベンチマークで GPT-3 を上回ります。このモデルは単一の GPU 上で実行できるため、LLM へのアクセスと研究の民主化に役立つと考えています。スケールのハイエンドでは、当社の 65B パラメータ モデルは、Chinchilla や PaLM-540B などの最高の大規模言語モデルと競合することもできます。",
      "font_name": "NimbusRomNo9L-Regu"
    },
    {
      "id": "para_p0_b5",
      "page_number": 0,
      "text": "1 Introduction",
      "block_bbox": {
        "x0": 70.865966796875,
        "y0": 437.7383117675781,
        "x1": 153.6796417236328,
        "y1": 453.2920227050781
      },
      "line_count": 1,
      "original_font_size": 12.0,
      "is_bold": true,
      "is_italic": false,
      "rotation": 0.0,
      "alignment": "center",
      "layout_block_id": "fa0b918b-cd03-4f23-a025-934cd43fa760",
      "category": "paragraph_title",
      "category_confidence": 0.9344824552536011,
      "font_name": "NimbusRomNo9L-Medi"
    },
    {
      "id": "para_p0_b6",
      "page_number": 0,
      "text": "Large Languages Models (LLMs) trained on mas- sive corpora of texts have shown their ability to per- form new tasks from textual instructions or from a few examples (Brown et al., 2020). These few-shot properties first appeared when scaling models to a sufficient size (Kaplan et al., 2020), resulting in a line of work that focuses on further scaling these models (Chowdhery et al., 2022; Rae et al., 2021). These efforts are based on the assumption that more parameters will lead to better performance. However, recent work from Hoffmann et al. (2022) shows that, for a given compute budget, the best performances are not achieved by the largest mod- els, but by smaller models trained on more data.\nThe objective of the scaling laws from Hoff- mann et al. (2022) is to determine how to best scale the dataset and model sizes for a particular training compute budget. However, this objective disregards the inference budget, which becomes critical when serving a language model at scale. In this context, given a target level of performance, the preferred model is not the fastest to train but the fastest at inference, and although it may be cheaper to train a large model to reach a certain level of",
      "block_bbox": {
        "x0": 70.52799987792969,
        "y0": 106.2885513305664,
        "x1": 291.04620361328125,
        "y1": 431.0650329589844
      },
      "line_count": 24,
      "original_font_size": 10.9,
      "is_bold": false,
      "is_italic": false,
      "rotation": 0.0,
      "alignment": "left",
      "layout_block_id": "7babb7cf-bfdb-485a-826d-deb371a494f9",
      "category": "text",
      "category_confidence": 0.9880727529525757,
      "translated_text": "膨大なテキストのコーパスで訓練された大規模言語モデル (LLM) は、テキストの指示またはいくつかの例から新しいタスクを実行する能力を示しています (Brown et al., 2020)。これらの少数ショットの特性は、モデルを十分なサイズにスケーリングするときに初めて現れ (Kaplan et al., 2020)、その結果、これらのモデルをさらにスケーリングすることに焦点を当てた一連の作業が行われました (Chowdhery et al., 2022; Rae et al., 2021)。これらの取り組みは、パラメータが多いほどパフォーマンスが向上するという前提に基づいています。しかし、ホフマンらの最近の研究では、 (2022) は、特定のコンピューティング バジェットにおいて、最高のパフォーマンスは最大のモデルによって達成されるのではなく、より多くのデータでトレーニングされた小規模なモデルによって達成されることを示しています。\nホフマンらのスケーリング則の目的は次のとおりです。 (2022) は、特定のトレーニング コンピューティング予算に合わせてデータセットとモデルのサイズを最適にスケーリングする方法を決定することです。ただし、この目標では推論バジェットを無視しています。推論バジェットは、言語モデルを大規模に提供する場合に重要になります。この文脈では、目標レベルのパフォーマンスが与えられた場合、推奨されるモデルはトレーニングが最も速いモデルではなく、推論が最も速いモデルです。また、一定のレベルに達するために大規模なモデルをトレーニングした方がコストが安くなる場合もありますが、",
      "font_name": "NimbusRomNo9L-Regu"
    },
    {
      "id": "para_p0_b11",
      "page_number": 0,
      "text": "Unlike Chinchilla, PaLM, or GPT-3, we only use publicly available data, making our work com- patible with open-sourcing, while most existing models rely on data which is either not publicly available or undocumented (e.g. “Books – 2TB” or “Social media conversations”). There exist some\nexceptions, notably OPT (Zhang et al., 2022), GPT-NeoX (Black et al., 2022), BLOOM (Scao et al., 2022) and GLM (Zeng et al., 2022), but none that are competitive with PaLM-62B or Chinchilla.",
      "block_bbox": {
        "x0": 304.6910095214844,
        "y0": 200.4995574951172,
        "x1": 526.3516235351562,
        "y1": 335.5870361328125
      },
      "line_count": 10,
      "original_font_size": 10.9,
      "is_bold": false,
      "is_italic": false,
      "rotation": 0.0,
      "alignment": "left",
      "layout_block_id": "3969a930-3a1e-4e44-af2a-158cda68fc37",
      "category": "text",
      "category_confidence": 0.9859020709991455,
      "translated_text": "Chinchilla、PaLM、または GPT-3 とは異なり、私たちは公開されているデータのみを使用するため、私たちの作業はオープンソースと互換性がありますが、既存のモデルのほとんどは公開されていない、または文書化されていないデータ (例: 「書籍 – 2TB」または「ソーシャル メディアでの会話」) に依存しています。いくつか存在します\n例外は、特に OPT (Zhang et al., 2022)、GPT-NeoX (Black et al., 2022)、BLOOM (Scao et al., 2022) および GLM (Zeng et al., 2022) ですが、PaLM-62B や Chinchilla と競合するものはありません。",
      "font_name": "NimbusRomNo9L-Regu"
    },
    {
      "id": "para_p0_b13",
      "page_number": 0,
      "text": "In the rest of this paper, we present an overview of the modifications we made to the transformer architecture (Vaswani et al., 2017), as well as our training method. We then report the performance of our models and compare with others LLMs on a set of standard benchmarks. Finally, we expose some of the biases and toxicity encoded in our models, using some of the most recent benchmarks from the responsible AI community.",
      "block_bbox": {
        "x0": 306.1419982910156,
        "y0": 67.6905517578125,
        "x1": 525.772216796875,
        "y1": 189.23001098632812
      },
      "line_count": 9,
      "original_font_size": 10.9,
      "is_bold": false,
      "is_italic": false,
      "rotation": 0.0,
      "alignment": "left",
      "layout_block_id": "b1c7b4ef-c45c-4b8f-932e-5c71a11df7f3",
      "category": "text",
      "category_confidence": 0.9853712320327759,
      "translated_text": "このペーパーの残りの部分では、変圧器アーキテクチャ (Vaswani et al.、2017) に加えた変更の概要と、トレーニング方法について説明します。次に、モデルのパフォーマンスをレポートし、一連の標準ベンチマークで他の LLM と比較します。最後に、責任ある AI コミュニティからの最新のベンチマークのいくつかを使用して、モデルにエンコードされたバイアスと毒性の一部を明らかにします。",
      "font_name": "NimbusRomNo9L-Regu"
    },
    {
      "id": "para_p0_b8",
      "page_number": 0,
      "text": "∗ Equal contribution. Correspondence: {htouvron, thibautlav,gizacard,egrave,glample}@meta.com 1 https://github.com/facebookresearch/llama",
      "block_bbox": {
        "x0": 70.86599731445312,
        "y0": 69.28990173339844,
        "x1": 291.375,
        "y1": 100.07640838623047
      },
      "line_count": 4,
      "original_font_size": 9.0,
      "is_bold": false,
      "is_italic": false,
      "rotation": 0.0,
      "alignment": "left",
      "layout_block_id": "1064bc7c-3f16-440f-a323-01497ab71c25",
      "category": "footnote",
      "category_confidence": 0.7735484838485718,
      "font_name": "Inconsolatazi4-Regular"
    }
  ]
}